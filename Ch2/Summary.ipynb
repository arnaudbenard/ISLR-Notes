{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Summary\n",
    "\n",
    "\n",
    "## What is statistical learning\n",
    "\n",
    "- General equation: $Y =f(X)+\\epsilon$\n",
    "    - $X$: Input variables, also called predictors, independent variables, features\n",
    "    - $Y$: Response, dependent variable\n",
    "    - $\\epsilon$: Random error term (independant from $X$ and $mean(\\epsilon) = 0$)\n",
    "    \n",
    "\n",
    "In essence, statistical learning refers to a set of approaches for estimating $f$.\n",
    "\n",
    "\n",
    "### Why estimate f?\n",
    "\n",
    "For prediction and inference\n",
    "\n",
    "#### Prediction\n",
    "\n",
    "In many situations, a set of inputs X are readily available, but the output Y cannot be easily obtained. In this setting, since the error term averages to zero, we can predict Y using:\n",
    "\n",
    "$\\hat{Y} = \\hat{f}(X)$\n",
    "- $\\hat{f}$ is the estimate for $f$ (treated as *black box*)\n",
    "- $\\hat{Y}$ is the prediction for $Y$\n",
    "\n",
    "The accuracy of $\\hat{Y}$ as a prediction for $Y$ depends on two quantities, which we will call the **reducible** error and the **irreducible** error. \n",
    "\n",
    "One error is **reducible** because we can potentially improve the accuracy of $\\hat{f}$ by using the most appropriate statistical learning technique to estimate $f$.\n",
    "\n",
    "The second is **irreducible** because even with a perfect estimate ($\\hat{Y} = f(X)$), $Y$ is still depending on $\\epsilon$ (which is not predicatble using $X$).\n",
    "\n",
    "The focus of this book is on techniques for estimating $f$ with the aim of minimizing the reducible error. It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for $Y$ . This bound is almost always unknown in practice.\n",
    "\n",
    "#### Inference\n",
    "\n",
    "Our goal with inference is not to make prediction but understanding the relationship between $X$ and $Y$. You might be interesting in answering the following questions: \n",
    "\n",
    "- Which predictors are associated with the response? \n",
    "- What is the relationship between the response and each predictor?\n",
    "- Can the relationship between Y and each predictor be adequately sum- marized using a linear equation, or is the relationship more complicated?\n",
    "\n",
    "### How do we estimate f?\n",
    "\n",
    "Our goal is to apply a statistical learning method to the observations (training data) in order to estimate the unknown function f. We want to find a function $\\hat{f}$ such that $Y ≈\\hat{f}(X)$ for any observation $(X,Y)$. There are two type of approaches: **parametric** or **non-parametric**.\n",
    "\n",
    "#### Parametric methods\n",
    "\n",
    "Parametric methods involve a two-step model-based approach.\n",
    "\n",
    "1. Make an assumption about the functional form, or shape, of $f$. For example we could assume that $f$ is linear: $f(X) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p (2.4)$\n",
    "\n",
    "2. Apply a procedure to estimate the paramters (finding $\\beta_0,\\beta_1,...,\\beta_p$)\n",
    "\n",
    "Assuming a parametric form for $f$ simplifies the problem of estimating $f$ because it is generally much easier to estimate a set of parameters, such as $\\beta_0,\\beta_1,...,\\beta_p$ in the linear model (2.4), than it is to fit an entirely arbitrary function $f$.\n",
    "\n",
    "The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of $f$. If the chosen model is too far from the true $f$, then our estimate will be poor.\n",
    "\n",
    "Assuming a **flexible** parametric form improves the estimate of $f$, but it requires estimating a greater number of parameters ($\\beta_i$). Also it might lead to **overfitting**, which means that the estimate follows the errors (noise) too closely.\n",
    "\n",
    "#### Non-parametric methods\n",
    "\n",
    "Non-parametric methods do not make explicit assumptions about the functional form of $f$, they have the potential to accurately fit a wider range of possible shapes for $f$.\n",
    "\n",
    "\n",
    "#### The trade-off between prediction accuracy and model interpretability\n",
    "\n",
    "![Tradeoff](./2.7.png)\n",
    "*A representation of the tradeoff between flexibility and interpretability, using different statistical learning methods. In general, as the flexibility of a method increases, its interpretability decreases.*\n",
    "\n",
    "\n",
    "Why would we ever choose to use a more restrictive method instead of a very flexible approach? \n",
    "\n",
    "- If we are mainly interested in inference, then restrictive models are much more interpretable.\n",
    "- If we are only interested in prediction, and the interpretability of the predictive model is simply not of interest.\n",
    "\n",
    "\n",
    "In the prediction setting, we might expect that it will be best to use the most flexible model available. Surprisingly, this is not always the case! **We will often obtain more accurate predictions using a less flexible method**. This phenomenon, which may seem counterintuitive at first glance, has to do with the **potential for overfitting in highly flexible methods.** \n",
    "\n",
    "#### Supervised vs unsupervised learning\n",
    "\n",
    "*Supervised:* \n",
    "- For each observation of the predictor measurement(s) $x_i, i = 1,...,n$ there is an associated response measurement $y_i$. We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference). \n",
    "\n",
    "*Unsupervised:*\n",
    "- For every observation $i = 1,...,n$, we observe a vector of measurements $xi$ but no associated response $yi$. In this setting, we are in some sense **working blind**; the situation is referred to as unsupervised because we lack a response variable that can supervise our analysis.One statistical learning tool that we may use is cluster analysis, or clustering. \n",
    "\n",
    "*Semi-supervised (not in the book)*\n",
    "- We have a set of $n$ observations. For $m$ of the observations, where $m < n$, we have both predictor measurements and a response measurement. For the remaining $n − m$ observations, we have predictor measurements but no response measurement.\n",
    "\n",
    "#### Regression vs classification problem\n",
    "\n",
    "We tend to refer to problems with a **quantitative** response as **regression** problems, while those involving a **qualitative** response are often referred to as **classification** problems.\n",
    "\n",
    "### Assessing model accuracy\n",
    "\n",
    "There is no free lunch in statistics: no one method dominates all others over all possible data sets. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.\n",
    "\n",
    "#### Measuring the quality of the fit\n",
    "\n",
    "In order to evaluate the performance of a statistical learning method on\n",
    "a given data set, we need some way to measure how well its predictions actually match the observed data. In the regression setting, the\n",
    "most commonly-used measure is the **mean squared error (MSE)**, given by:\n",
    "\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2 $\n",
    "\n",
    "The MSE\n",
    "will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
